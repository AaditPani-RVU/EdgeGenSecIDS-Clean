{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2e074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775a4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# === Prototypical Network Backbone ===\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, input_dim=78, embedding_dim=64):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# === Episode Sampling Function ===\n",
    "def sample_episode(data_dict, n_way, k_shot, q_query):\n",
    "    selected_classes = random.sample(list(data_dict.keys()), n_way)\n",
    "    support_x, support_y = [], []\n",
    "    query_x, query_y = [], []\n",
    "\n",
    "    for i, cls in enumerate(selected_classes):\n",
    "        samples = data_dict[cls]\n",
    "        selected = random.sample(samples, k_shot + q_query)\n",
    "        support = selected[:k_shot]\n",
    "        query = selected[k_shot:]\n",
    "\n",
    "        support_x.extend([x[0] for x in support])\n",
    "        support_y.extend([i] * k_shot)\n",
    "        query_x.extend([x[0] for x in query])\n",
    "        query_y.extend([i] * q_query)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(support_x, dtype=torch.float32),\n",
    "        torch.tensor(support_y),\n",
    "        torch.tensor(query_x, dtype=torch.float32),\n",
    "        torch.tensor(query_y),\n",
    "    )\n",
    "\n",
    "# === Prototypical Loss ===\n",
    "def prototypical_loss(model, support_x, support_y, query_x, query_y):\n",
    "    embeddings_support = model(support_x)\n",
    "    embeddings_query = model(query_x)\n",
    "\n",
    "    prototypes = []\n",
    "    for cls in torch.unique(support_y):\n",
    "        cls_embeddings = embeddings_support[support_y == cls]\n",
    "        prototypes.append(cls_embeddings.mean(0))\n",
    "    prototypes = torch.stack(prototypes)\n",
    "\n",
    "    dists = torch.cdist(embeddings_query, prototypes)\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)\n",
    "    loss = F.nll_loss(log_p_y, query_y)\n",
    "    acc = (log_p_y.argmax(1) == query_y).float().mean().item()\n",
    "    return loss, acc\n",
    "\n",
    "# === Training Loop ===\n",
    "def train_fewshot(model, data_dict, episodes=200, n_way=5, k_shot=5, q_query=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for ep in range(episodes):\n",
    "        support_x, support_y, query_x, query_y = sample_episode(data_dict, n_way, k_shot, q_query)\n",
    "        loss, acc = prototypical_loss(model, support_x, support_y, query_x, query_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"[Ep {ep}] Loss: {loss.item():.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "# === üîÅ Example Usage (after loading your preprocessed data_dict) ===\n",
    "# Format: data_dict = {'SQLi': [(feature_vec1,), (feature_vec2,), ...], 'XSS': [...], ...}\n",
    "# Include both real and synthetic samples in each class list.\n",
    "# train_fewshot(ProtoNet(), data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b042814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SQLi: Saved 0 real and 0 synthetic samples\n",
      "‚úÖ XSS: Saved 0 real and 0 synthetic samples\n",
      "‚úÖ Heartbleed: Saved 0 real and 0 synthetic samples\n",
      "‚úÖ Infiltration: Saved 0 real and 0 synthetic samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_path = \"DATA/CICIDS2017_Full_With_Synthetic.csv\"\n",
    "output_dir = \"DATA/RARE_CLASSES\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "rare_attacks = [\"SQLi\", \"XSS\", \"Heartbleed\", \"Infiltration\"]\n",
    "\n",
    "# Load master dataset\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Optional: if there's no 'Synthetic' column, infer from Label (e.g., \"SQLi_SYN\")\n",
    "def is_synthetic(label):\n",
    "    if isinstance(label, str):\n",
    "        return \"SYN\" in label or \"synthetic\" in label.lower()\n",
    "    return False\n",
    "\n",
    "\n",
    "for attack in rare_attacks:\n",
    "    # Ensure \"Label\" is string for string operations\n",
    "    label_str = df[\"Label\"].astype(str)\n",
    "\n",
    "    # Real rows\n",
    "    real_rows = df[(label_str == attack) & (~label_str.apply(is_synthetic))]\n",
    "    # Synthetic rows (either different label or inferred)\n",
    "    synthetic_rows = df[(label_str.str.contains(attack)) & (label_str.apply(is_synthetic))]\n",
    "\n",
    "    # Save\n",
    "    real_path = os.path.join(output_dir, f\"{attack}_real.csv\")\n",
    "    syn_path = os.path.join(output_dir, f\"{attack}_synthetic.csv\")\n",
    "\n",
    "    real_rows.drop(columns=[\"Label\"], errors='ignore').to_csv(real_path, index=False)\n",
    "    synthetic_rows.drop(columns=[\"Label\"], errors='ignore').to_csv(syn_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ {attack}: Saved {len(real_rows)} real and {len(synthetic_rows)} synthetic samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4188b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLi: 0 samples\n",
      "XSS: 0 samples\n",
      "Heartbleed: 0 samples\n",
      "Infiltration: 0 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "attack_classes = [\"SQLi\", \"XSS\", \"Heartbleed\", \"Infiltration\"]\n",
    "base_dir = \"DATA/RARE_CLASSES\"  # change this to your actual folder\n",
    "\n",
    "def load_attack_class_samples(cls):\n",
    "    real_path = os.path.join(base_dir, f\"{cls}_real.csv\")\n",
    "    syn_path = os.path.join(base_dir, f\"{cls}_synthetic.csv\")\n",
    "\n",
    "    real_df = pd.read_csv(real_path)\n",
    "    syn_df = pd.read_csv(syn_path)\n",
    "\n",
    "    combined_df = pd.concat([real_df, syn_df])\n",
    "    combined_df = combined_df.sample(frac=1).reset_index(drop=True)  # shuffle\n",
    "    return [(row.values,) for _, row in combined_df.iterrows()]\n",
    "\n",
    "# Build the dictionary\n",
    "data_dict = {}\n",
    "for attack in attack_classes:\n",
    "    data_dict[attack] = load_attack_class_samples(attack)\n",
    "\n",
    "# Confirm structure\n",
    "for cls, samples in data_dict.items():\n",
    "    print(f\"{cls}: {len(samples)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aeffab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "BENIGN                        2084030\n",
      "DoS Hulk                       231073\n",
      "PortScan                       158930\n",
      "DDoS                           128027\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7938\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Web Attack ÔøΩ Brute Force         1507\n",
      "Web Attack ÔøΩ XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack ÔøΩ Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"DATA/CICIDS2017_Multiclass_Merged.csv\")\n",
    "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces from column names\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0758afd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 21 samples to DATA/RARE_CLASSES\\SQLi_real.csv\n",
      "‚úÖ Saved 652 samples to DATA/RARE_CLASSES\\XSS_real.csv\n",
      "‚úÖ Saved 11 samples to DATA/RARE_CLASSES\\Heartbleed_real.csv\n",
      "‚úÖ Saved 36 samples to DATA/RARE_CLASSES\\Infiltration_real.csv\n"
     ]
    }
   ],
   "source": [
    "# Use the already loaded and cleaned df from previous cells\n",
    "\n",
    "output_dir = \"DATA/RARE_CLASSES\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ensure column names are stripped of spaces\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# === Normalize label column (some have special characters) ===\n",
    "df[\"Label\"] = df[\"Label\"].astype(str).str.replace(\"ÔøΩ\", \"-\", regex=False).str.strip()\n",
    "\n",
    "# === Rare classes to extract ===\n",
    "rare_classes = {\n",
    "    \"SQLi\": \"Web Attack - Sql Injection\",\n",
    "    \"XSS\": \"Web Attack - XSS\",\n",
    "    \"Heartbleed\": \"Heartbleed\",\n",
    "    \"Infiltration\": \"Infiltration\"\n",
    "}\n",
    "\n",
    "# === Extract and save each class ===\n",
    "for short_name, full_label in rare_classes.items():\n",
    "    class_df = df[df[\"Label\"] == full_label]\n",
    "    out_path = os.path.join(output_dir, f\"{short_name}_real.csv\")\n",
    "    class_df.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ Saved {len(class_df)} samples to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27ef054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# üìÇ Base directory where real + synthetic CSVs live\n",
    "base_dir = \"DATA/RARE_CLASSES\"\n",
    "attack_classes = [\"SQLi\", \"XSS\", \"Heartbleed\", \"Infiltration\"]\n",
    "\n",
    "# üîÅ Load support + query samples for few-shot learning\n",
    "def load_attack_class_samples(cls):\n",
    "    real_path = os.path.join(base_dir, f\"{cls}_real.csv\")\n",
    "    syn_path = os.path.join(base_dir, f\"{cls}_synthetic.csv\")\n",
    "\n",
    "    real_df = pd.read_csv(real_path)\n",
    "    syn_df = pd.read_csv(syn_path)\n",
    "\n",
    "    # üîÄ Shuffle both\n",
    "    real_df = real_df.sample(frac=1).reset_index(drop=True)\n",
    "    syn_df = syn_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return real_df, syn_df\n",
    "\n",
    "# üéØ Sample few-shot episode\n",
    "def sample_episode(n_way=4, k_shot=5, q_query=10):\n",
    "    support_set = []\n",
    "    query_set = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "\n",
    "    label_map = {cls: i for i, cls in enumerate(attack_classes)}\n",
    "\n",
    "    for cls in random.sample(attack_classes, n_way):\n",
    "        real_df, syn_df = load_attack_class_samples(cls)\n",
    "\n",
    "        # Ensure we have enough samples\n",
    "        k_real = k_shot // 2\n",
    "        k_syn = k_shot - k_real\n",
    "\n",
    "        # Support set: half real + half synthetic\n",
    "        support_real = real_df[:k_real]\n",
    "        support_syn = syn_df[:k_syn]\n",
    "        support_combined = pd.concat([support_real, support_syn])\n",
    "\n",
    "        # Query set: all real\n",
    "        query_real = real_df[k_real:k_real + q_query]\n",
    "\n",
    "        # Append to episode\n",
    "        support_set.append(support_combined)\n",
    "        query_set.append(query_real)\n",
    "        support_labels.extend([label_map[cls]] * k_shot)\n",
    "        query_labels.extend([label_map[cls]] * q_query)\n",
    "\n",
    "    # üîÑ Final tensors\n",
    "    support_df = pd.concat(support_set).drop(columns=[\"Label\"])\n",
    "    query_df = pd.concat(query_set).drop(columns=[\"Label\"])\n",
    "\n",
    "    X_support = torch.tensor(support_df.values, dtype=torch.float32)\n",
    "    y_support = torch.tensor(support_labels)\n",
    "    X_query = torch.tensor(query_df.values, dtype=torch.float32)\n",
    "    y_query = torch.tensor(query_labels)\n",
    "\n",
    "    return X_support, y_support, X_query, y_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a78bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# üß† Lightweight Encoder for 78D CICIDS-style features\n",
    "class ProtoNetEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=78, hidden_dim=128, output_dim=64):\n",
    "        super(ProtoNetEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# üß™ Prototypical Loss Function\n",
    "def prototypical_loss(X_support, y_support, X_query, y_query, encoder):\n",
    "    # Encode\n",
    "    z_support = encoder(X_support)\n",
    "    z_query = encoder(X_query)\n",
    "\n",
    "    # Get unique classes\n",
    "    classes = torch.unique(y_support)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Compute prototypes (mean of support embeddings per class)\n",
    "    prototypes = torch.stack([z_support[y_support == cls].mean(0) for cls in classes])\n",
    "\n",
    "    # Compute squared Euclidean distance from query to prototypes\n",
    "    dists = torch.cdist(z_query, prototypes, p=2)\n",
    "\n",
    "    # Predicted labels = nearest prototype\n",
    "    y_pred = dists.argmin(dim=1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = (y_pred == y_query).float().mean().item()\n",
    "\n",
    "    return acc, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab4090d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded DATA/RARE_CLASSES/SQLi_real.csv: shape=(21, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/SQLi_synthetic.csv: shape=(1000, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/XSS_real.csv: shape=(652, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/XSS_synthetic.csv: shape=(1000, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/Heartbleed_real.csv: shape=(11, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/Heartbleed_synthetic.csv: shape=(1000, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/Infiltration_real.csv: shape=(36, 79)\n",
      "‚úÖ Loaded DATA/RARE_CLASSES/Infiltration_synthetic.csv: shape=(1000, 79)\n",
      "üîç Shape before dropna: (4720, 79)\n",
      "‚úÖ Shape after dropna: (4720, 79)\n",
      "[Episode 0] Accuracy: 0.7250 | Loss: 2.2058\n",
      "[Episode 10] Accuracy: 1.0000 | Loss: 0.0789\n",
      "[Episode 20] Accuracy: 1.0000 | Loss: 0.0144\n",
      "[Episode 30] Accuracy: 0.9750 | Loss: 0.0839\n",
      "[Episode 40] Accuracy: 1.0000 | Loss: 0.0023\n",
      "[Episode 50] Accuracy: 1.0000 | Loss: 0.0029\n",
      "[Episode 60] Accuracy: 1.0000 | Loss: 0.0020\n",
      "[Episode 70] Accuracy: 1.0000 | Loss: 0.0013\n",
      "[Episode 80] Accuracy: 1.0000 | Loss: 0.0015\n",
      "[Episode 90] Accuracy: 1.0000 | Loss: 0.0007\n",
      "‚úÖ Saved encoder as protonet_encoder.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === 1. Load Real + Synthetic for Rare Classes ===\n",
    "rare_classes = [\"SQLi\", \"XSS\", \"Heartbleed\", \"Infiltration\"]\n",
    "dfs = []\n",
    "\n",
    "for cls in rare_classes:\n",
    "    for kind in [\"real\", \"synthetic\"]:\n",
    "        path = f\"DATA/RARE_CLASSES/{cls}_{kind}.csv\"\n",
    "        if os.path.exists(path):\n",
    "            df_part = pd.read_csv(path)\n",
    "            df_part.columns = [c.strip() for c in df_part.columns]\n",
    "            df_part[\"Label\"] = cls\n",
    "            if df_part.shape[0] == 0:\n",
    "                print(f\"‚ö†Ô∏è Skipped empty file: {path}\")\n",
    "                continue\n",
    "            print(f\"‚úÖ Loaded {path}: shape={df_part.shape}\")\n",
    "            dfs.append(df_part)\n",
    "        else:\n",
    "            print(f\"‚ùå Missing file: {path}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"‚ùå No data loaded. Ensure real and synthetic CSVs exist.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "feature_cols = [col for col in df.columns if col != \"Label\"]\n",
    "\n",
    "# === 2. Data Cleaning ===\n",
    "for col in feature_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(f\"üîç Shape before dropna: {df.shape}\")\n",
    "df.dropna(subset=feature_cols, inplace=True)\n",
    "print(f\"‚úÖ Shape after dropna: {df.shape}\")\n",
    "\n",
    "# === 3. Encode Labels ===\n",
    "label_map = {name: i for i, name in enumerate(sorted(df[\"Label\"].unique()))}\n",
    "df[\"Label\"] = df[\"Label\"].map(label_map)\n",
    "\n",
    "# === 4. Scale Features ===\n",
    "scaler = StandardScaler()\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "# === 5. Encoder with He + Orthogonal Init ===\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=78, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.orthogonal_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc2(self.relu(self.fc1(x))))\n",
    "\n",
    "# === 6. Few-Shot Episode Sampler ===\n",
    "def sample_episode(df, n_way=4, k_shot=5, q_query=10):\n",
    "    support_x, support_y, query_x, query_y = [], [], [], []\n",
    "    selected_classes = sorted(df[\"Label\"].unique())[:n_way]\n",
    "\n",
    "    for cls in selected_classes:\n",
    "        class_df = df[df[\"Label\"] == cls]\n",
    "        n_samples = k_shot + q_query\n",
    "        replace = len(class_df) < n_samples\n",
    "        samples = class_df.sample(n=n_samples, replace=replace)\n",
    "        support = samples.iloc[:k_shot]\n",
    "        query = samples.iloc[k_shot:]\n",
    "\n",
    "        support_x.append(support[feature_cols].values)\n",
    "        support_y.append([cls] * k_shot)\n",
    "        query_x.append(query[feature_cols].values)\n",
    "        query_y.append([cls] * q_query)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(np.vstack(support_x), dtype=torch.float32),\n",
    "        torch.tensor(np.concatenate(support_y), dtype=torch.long),\n",
    "        torch.tensor(np.vstack(query_x), dtype=torch.float32),\n",
    "        torch.tensor(np.concatenate(query_y), dtype=torch.long),\n",
    "    )\n",
    "\n",
    "# === 7. ProtoNet Loss ===\n",
    "def prototypical_loss(encoder, support_x, support_y, query_x, query_y):\n",
    "    emb_support = encoder(support_x)\n",
    "    emb_query = encoder(query_x)\n",
    "    prototypes = torch.stack([emb_support[support_y == c].mean(0) for c in torch.unique(support_y)])\n",
    "    logits = -torch.cdist(emb_query, prototypes)\n",
    "    loss = F.cross_entropy(logits, query_y)\n",
    "    acc = (logits.argmax(dim=1) == query_y).float().mean().item()\n",
    "    return loss, acc\n",
    "\n",
    "# === 8. Train ProtoNet ===\n",
    "encoder = Encoder(input_dim=len(feature_cols))\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "\n",
    "for episode in range(100):\n",
    "    support_x, support_y, query_x, query_y = sample_episode(df)\n",
    "    optimizer.zero_grad()\n",
    "    loss, acc = prototypical_loss(encoder, support_x, support_y, query_x, query_y)\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"[Episode {episode}] ‚ö†Ô∏è NaN Loss ‚Äì skipping.\")\n",
    "        continue\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"[Episode {episode}] Accuracy: {acc:.4f} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === 9. Save Encoder ===\n",
    "torch.save(encoder.state_dict(), \"protonet_encoder.pt\")\n",
    "print(\"‚úÖ Saved encoder as protonet_encoder.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97361340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] D_loss: 1.4073 | G_loss: 0.7243\n",
      "[500] D_loss: 0.7796 | G_loss: 1.3943\n",
      "[1000] D_loss: 0.6397 | G_loss: 1.8202\n",
      "[1500] D_loss: 0.7552 | G_loss: 1.3358\n",
      "[2000] D_loss: 0.8220 | G_loss: 1.2485\n",
      "[2500] D_loss: 0.7672 | G_loss: 1.2269\n",
      "[3000] D_loss: 0.8158 | G_loss: 1.6233\n",
      "[3500] D_loss: 0.4855 | G_loss: 1.3848\n",
      "[4000] D_loss: 0.5399 | G_loss: 1.4846\n",
      "[4500] D_loss: 0.6570 | G_loss: 1.7283\n",
      "‚úÖ Saved 1000 synthetic samples to DATA/RARE_CLASSES/SQLi_synthetic.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === CONFIG ===\n",
    "CLASS_NAME = \"SQLi\"  # Change to \"XSS\", \"Heartbleed\", \"Infiltration\" as needed\n",
    "REAL_CSV = f\"DATA/RARE_CLASSES/{CLASS_NAME}_real.csv\"\n",
    "SYN_CSV = f\"DATA/RARE_CLASSES/{CLASS_NAME}_synthetic.csv\"\n",
    "EPOCHS = 5000\n",
    "LATENT_DIM = 32\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# === 1. Load and Normalize Real Data ===\n",
    "df = pd.read_csv(REAL_CSV)\n",
    "feature_cols = [c for c in df.columns if c != \"Label\"]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[feature_cols])\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# === 2. Define Generator & Discriminator ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# === 3. Init Models ===\n",
    "G = Generator(LATENT_DIM, X.shape[1])\n",
    "D = Discriminator(X.shape[1])\n",
    "g_opt = optim.Adam(G.parameters(), lr=0.001)\n",
    "d_opt = optim.Adam(D.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# === 4. Training Loop ===\n",
    "for epoch in range(EPOCHS):\n",
    "    # === Train Discriminator ===\n",
    "    real_idx = torch.randint(0, X.shape[0], (BATCH_SIZE,))\n",
    "    real_samples = X[real_idx]\n",
    "    z = torch.randn(BATCH_SIZE, LATENT_DIM)\n",
    "    fake_samples = G(z).detach()\n",
    "    \n",
    "    d_real = D(real_samples)\n",
    "    d_fake = D(fake_samples)\n",
    "\n",
    "    d_loss = loss_fn(d_real, torch.ones_like(d_real)) + loss_fn(d_fake, torch.zeros_like(d_fake))\n",
    "    d_opt.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_opt.step()\n",
    "\n",
    "    # === Train Generator ===\n",
    "    z = torch.randn(BATCH_SIZE, LATENT_DIM)\n",
    "    gen_samples = G(z)\n",
    "    d_gen = D(gen_samples)\n",
    "    g_loss = loss_fn(d_gen, torch.ones_like(d_gen))\n",
    "    g_opt.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_opt.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"[{epoch}] D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# === 5. Generate and Save Synthetic Samples ===\n",
    "num_samples = 1000  # Or match the real count if very small\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, LATENT_DIM)\n",
    "    syn_samples = G(z).numpy()\n",
    "\n",
    "# Inverse scale before saving\n",
    "syn_df = pd.DataFrame(scaler.inverse_transform(syn_samples), columns=feature_cols)\n",
    "syn_df.to_csv(SYN_CSV, index=False)\n",
    "print(f\"‚úÖ Saved {num_samples} synthetic samples to {SYN_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cec3eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Support set regenerated and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "rare_classes = [\"SQLi\", \"XSS\", \"Heartbleed\", \"Infiltration\"]\n",
    "support_real_per_class = 5\n",
    "support_synthetic_per_class = 5\n",
    "\n",
    "real_dir = \"DATA/RARE_CLASSES\"\n",
    "synthetic_dir = \"DATA/RARE_CLASSES\"\n",
    "\n",
    "support_x = []\n",
    "support_y = []\n",
    "label_map = {cls: i for i, cls in enumerate(rare_classes)}\n",
    "\n",
    "for cls in rare_classes:\n",
    "    label_id = label_map[cls]\n",
    "\n",
    "    # Load real samples\n",
    "    real_path = os.path.join(real_dir, f\"{cls}_real.csv\")\n",
    "    if os.path.exists(real_path):\n",
    "        df_real = pd.read_csv(real_path).dropna()\n",
    "        df_real[\"Label\"] = label_id\n",
    "        df_real = df_real.sample(n=min(support_real_per_class, len(df_real)), random_state=42)\n",
    "        support_x.append(df_real.drop(columns=[\"Label\"]).values)\n",
    "        support_y.append([label_id] * len(df_real))\n",
    "\n",
    "    # Load synthetic samples\n",
    "    syn_path = os.path.join(synthetic_dir, f\"{cls}_synthetic.csv\")\n",
    "    if os.path.exists(syn_path):\n",
    "        df_syn = pd.read_csv(syn_path).dropna()\n",
    "        df_syn[\"Label\"] = label_id\n",
    "        df_syn = df_syn.sample(n=min(support_synthetic_per_class, len(df_syn)), random_state=42)\n",
    "        support_x.append(df_syn.drop(columns=[\"Label\"]).values)\n",
    "        support_y.append([label_id] * len(df_syn))\n",
    "\n",
    "# Stack and save\n",
    "support_x = torch.tensor(np.vstack(support_x), dtype=torch.float32)\n",
    "support_y = torch.tensor(np.concatenate(support_y), dtype=torch.long)\n",
    "\n",
    "torch.save(support_x, \"DATA/RARE_CLASSES/support_x.pt\")\n",
    "torch.save(support_y, \"DATA/RARE_CLASSES/support_y.pt\")\n",
    "print(\"‚úÖ Support set regenerated and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

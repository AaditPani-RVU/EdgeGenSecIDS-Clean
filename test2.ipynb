{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18593c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted: SYN (11269 flows)\n",
      "✅ Extracted: FIN (2 flows)\n",
      "✅ Extracted: XMAS (11 flows)\n",
      "✅ Final dataset saved at: C:\\Users\\aadip\\Desktop\\internship\\EdgeGenSec\\CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scapy.all import rdpcap, TCP, IP\n",
    "import os\n",
    "\n",
    "# === Function to extract features from pcap ===\n",
    "def extract_flows_from_pcap(pcap_path, sublabel):\n",
    "    packets = rdpcap(pcap_path)\n",
    "    flows = {}\n",
    "    timestamps = {}\n",
    "    flag_counts = {}\n",
    "\n",
    "    for pkt in packets:\n",
    "        if IP in pkt and TCP in pkt:\n",
    "            fid = (pkt[IP].src, pkt[IP].dst, pkt[TCP].sport, pkt[TCP].dport)\n",
    "            if fid not in flows:\n",
    "                flows[fid] = []\n",
    "                timestamps[fid] = []\n",
    "                flag_counts[fid] = {\"SYN\": 0, \"ACK\": 0, \"RST\": 0, \"FIN\": 0}\n",
    "            flows[fid].append(len(pkt))\n",
    "            timestamps[fid].append(pkt.time)\n",
    "\n",
    "            flags = pkt[TCP].flags\n",
    "            flag_counts[fid][\"SYN\"] += int(flags & 0x02 != 0)\n",
    "            flag_counts[fid][\"ACK\"] += int(flags & 0x10 != 0)\n",
    "            flag_counts[fid][\"RST\"] += int(flags & 0x04 != 0)\n",
    "            flag_counts[fid][\"FIN\"] += int(flags & 0x01 != 0)\n",
    "\n",
    "    records = []\n",
    "    for fid, sizes in flows.items():\n",
    "        times = timestamps[fid]\n",
    "        if len(times) < 2:\n",
    "            continue\n",
    "        duration = max(times) - min(times)\n",
    "        total_pkts = len(sizes)\n",
    "        total_bytes = sum(sizes)\n",
    "        min_size = min(sizes)\n",
    "        max_size = max(sizes)\n",
    "        mean_size = sum(sizes) / total_pkts\n",
    "\n",
    "        fwd_pkts = bwd_pkts = total_pkts // 2  # approximation\n",
    "        flags = flag_counts[fid]\n",
    "\n",
    "        records.append([\n",
    "            duration, total_pkts, total_bytes,\n",
    "            min_size, max_size, mean_size,\n",
    "            fwd_pkts, bwd_pkts,\n",
    "            flags[\"SYN\"], flags[\"ACK\"], flags[\"RST\"], flags[\"FIN\"],\n",
    "            sublabel\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(records, columns=[\n",
    "        \"Duration\", \"TotalPackets\", \"TotalBytes\",\n",
    "        \"MinPktSize\", \"MaxPktSize\", \"MeanPktSize\",\n",
    "        \"FwdPkts\", \"BwdPkts\",\n",
    "        \"SYN_Count\", \"ACK_Count\", \"RST_Count\", \"FIN_Count\",\n",
    "        \"SubLabel\"\n",
    "    ])\n",
    "\n",
    "\n",
    "# === File paths (update here as needed) ===\n",
    "base_path = \"/home/aadip/EdgeGenSec\"  # or use absolute Windows path if local\n",
    "pcaps = {\n",
    "    \"SYN\": \"portscan_syn.pcap\",\n",
    "    \"FIN\": \"portscan_fin.pcap\",\n",
    "    \"XMAS\": \"portscan_xmas.pcap\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# === Extract all new flows ===\n",
    "df_all = []\n",
    "for mode, pcap_file in pcaps.items():\n",
    "    df = extract_flows_from_pcap(pcap_file, sublabel=mode)\n",
    "    df_all.append(df)\n",
    "    print(f\"✅ Extracted: {mode} ({len(df)} flows)\")\n",
    "df_portscan_all = pd.concat(df_all, ignore_index=True)\n",
    "df_portscan_all[\"Label\"] = \"PortScan\"\n",
    "\n",
    "# === Load existing base dataset (update path if needed) ===\n",
    "df_base = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k.csv\")\n",
    "\n",
    "# === Append and Save ===\n",
    "# === Append and Save ===\n",
    "df_final = pd.concat([df_base, df_portscan_all], ignore_index=True)\n",
    "\n",
    "# ✅ Explicit path for Windows user\n",
    "output_path = r\"C:\\Users\\aadip\\Desktop\\internship\\EdgeGenSec\\CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Final dataset saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7949c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Destination Port  Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
      "0            1083.0           13.0                1.0                     1.0   \n",
      "1              80.0      4306913.0                6.0                     0.0   \n",
      "2              80.0      5137816.0                3.0                     1.0   \n",
      "3              80.0      3980908.0                5.0                     0.0   \n",
      "4              22.0     12851642.0               21.0                    32.0   \n",
      "\n",
      "   Total Length of Fwd Packets  Total Length of Bwd Packets  \\\n",
      "0                          0.0                          6.0   \n",
      "1                         36.0                          0.0   \n",
      "2                          0.0                          0.0   \n",
      "3                         30.0                          0.0   \n",
      "4                       2008.0                       2745.0   \n",
      "\n",
      "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
      "0                    0.0                    0.0                0.000000   \n",
      "1                    6.0                    6.0                6.000000   \n",
      "2                    0.0                    0.0                0.000000   \n",
      "3                    6.0                    6.0                6.000000   \n",
      "4                  640.0                    0.0               95.619048   \n",
      "\n",
      "   Fwd Packet Length Std  ...  MinPktSize  MaxPktSize  MeanPktSize  FwdPkts  \\\n",
      "0               0.000000  ...         NaN         NaN          NaN      NaN   \n",
      "1               0.000000  ...         NaN         NaN          NaN      NaN   \n",
      "2               0.000000  ...         NaN         NaN          NaN      NaN   \n",
      "3               0.000000  ...         NaN         NaN          NaN      NaN   \n",
      "4             140.045163  ...         NaN         NaN          NaN      NaN   \n",
      "\n",
      "   BwdPkts  SYN_Count  ACK_Count  RST_Count  FIN_Count  SubLabel  \n",
      "0      NaN        NaN        NaN        NaN        NaN       NaN  \n",
      "1      NaN        NaN        NaN        NaN        NaN       NaN  \n",
      "2      NaN        NaN        NaN        NaN        NaN       NaN  \n",
      "3      NaN        NaN        NaN        NaN        NaN       NaN  \n",
      "4      NaN        NaN        NaN        NaN        NaN       NaN  \n",
      "\n",
      "[5 rows x 92 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58509 entries, 0 to 58508\n",
      "Data columns (total 92 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Destination Port             47227 non-null  float64\n",
      " 1   Flow Duration                47227 non-null  float64\n",
      " 2   Total Fwd Packets            47227 non-null  float64\n",
      " 3   Total Backward Packets       47227 non-null  float64\n",
      " 4   Total Length of Fwd Packets  47227 non-null  float64\n",
      " 5   Total Length of Bwd Packets  47227 non-null  float64\n",
      " 6   Fwd Packet Length Max        47227 non-null  float64\n",
      " 7   Fwd Packet Length Min        47227 non-null  float64\n",
      " 8   Fwd Packet Length Mean       47227 non-null  float64\n",
      " 9   Fwd Packet Length Std        47227 non-null  float64\n",
      " 10  Bwd Packet Length Max        47227 non-null  float64\n",
      " 11  Bwd Packet Length Min        47227 non-null  float64\n",
      " 12  Bwd Packet Length Mean       47227 non-null  float64\n",
      " 13  Bwd Packet Length Std        47227 non-null  float64\n",
      " 14  Flow Bytes/s                 47207 non-null  float64\n",
      " 15  Flow Packets/s               47227 non-null  float64\n",
      " 16  Flow IAT Mean                47227 non-null  float64\n",
      " 17  Flow IAT Std                 47227 non-null  float64\n",
      " 18  Flow IAT Max                 47227 non-null  float64\n",
      " 19  Flow IAT Min                 47227 non-null  float64\n",
      " 20  Fwd IAT Total                47227 non-null  float64\n",
      " 21  Fwd IAT Mean                 47227 non-null  float64\n",
      " 22  Fwd IAT Std                  47227 non-null  float64\n",
      " 23  Fwd IAT Max                  47227 non-null  float64\n",
      " 24  Fwd IAT Min                  47227 non-null  float64\n",
      " 25  Bwd IAT Total                47227 non-null  float64\n",
      " 26  Bwd IAT Mean                 47227 non-null  float64\n",
      " 27  Bwd IAT Std                  47227 non-null  float64\n",
      " 28  Bwd IAT Max                  47227 non-null  float64\n",
      " 29  Bwd IAT Min                  47227 non-null  float64\n",
      " 30  Fwd PSH Flags                47227 non-null  float64\n",
      " 31  Bwd PSH Flags                47227 non-null  float64\n",
      " 32  Fwd URG Flags                47227 non-null  float64\n",
      " 33  Bwd URG Flags                47227 non-null  float64\n",
      " 34  Fwd Header Length            47227 non-null  float64\n",
      " 35  Bwd Header Length            47227 non-null  float64\n",
      " 36  Fwd Packets/s                47227 non-null  float64\n",
      " 37  Bwd Packets/s                47227 non-null  float64\n",
      " 38  Min Packet Length            47227 non-null  float64\n",
      " 39  Max Packet Length            47227 non-null  float64\n",
      " 40  Packet Length Mean           47227 non-null  float64\n",
      " 41  Packet Length Std            47227 non-null  float64\n",
      " 42  Packet Length Variance       47227 non-null  float64\n",
      " 43  FIN Flag Count               47227 non-null  float64\n",
      " 44  SYN Flag Count               47227 non-null  float64\n",
      " 45  RST Flag Count               47227 non-null  float64\n",
      " 46  PSH Flag Count               47227 non-null  float64\n",
      " 47  ACK Flag Count               47227 non-null  float64\n",
      " 48  URG Flag Count               47227 non-null  float64\n",
      " 49  CWE Flag Count               47227 non-null  float64\n",
      " 50  ECE Flag Count               47227 non-null  float64\n",
      " 51  Down/Up Ratio                47227 non-null  float64\n",
      " 52  Average Packet Size          47227 non-null  float64\n",
      " 53  Avg Fwd Segment Size         47227 non-null  float64\n",
      " 54  Avg Bwd Segment Size         47227 non-null  float64\n",
      " 55  Fwd Header Length.1          47227 non-null  float64\n",
      " 56  Fwd Avg Bytes/Bulk           47227 non-null  float64\n",
      " 57  Fwd Avg Packets/Bulk         47227 non-null  float64\n",
      " 58  Fwd Avg Bulk Rate            47227 non-null  float64\n",
      " 59  Bwd Avg Bytes/Bulk           47227 non-null  float64\n",
      " 60  Bwd Avg Packets/Bulk         47227 non-null  float64\n",
      " 61  Bwd Avg Bulk Rate            47227 non-null  float64\n",
      " 62  Subflow Fwd Packets          47227 non-null  float64\n",
      " 63  Subflow Fwd Bytes            47227 non-null  float64\n",
      " 64  Subflow Bwd Packets          47227 non-null  float64\n",
      " 65  Subflow Bwd Bytes            47227 non-null  float64\n",
      " 66  Init_Win_bytes_forward       47227 non-null  float64\n",
      " 67  Init_Win_bytes_backward      47227 non-null  float64\n",
      " 68  act_data_pkt_fwd             47227 non-null  float64\n",
      " 69  min_seg_size_forward         47227 non-null  float64\n",
      " 70  Active Mean                  47227 non-null  float64\n",
      " 71  Active Std                   47227 non-null  float64\n",
      " 72  Active Max                   47227 non-null  float64\n",
      " 73  Active Min                   47227 non-null  float64\n",
      " 74  Idle Mean                    47227 non-null  float64\n",
      " 75  Idle Std                     47227 non-null  float64\n",
      " 76  Idle Max                     47227 non-null  float64\n",
      " 77  Idle Min                     47227 non-null  float64\n",
      " 78  Label                        58509 non-null  object \n",
      " 79  Duration                     11282 non-null  float64\n",
      " 80  TotalPackets                 11282 non-null  float64\n",
      " 81  TotalBytes                   11282 non-null  float64\n",
      " 82  MinPktSize                   11282 non-null  float64\n",
      " 83  MaxPktSize                   11282 non-null  float64\n",
      " 84  MeanPktSize                  11282 non-null  float64\n",
      " 85  FwdPkts                      11282 non-null  float64\n",
      " 86  BwdPkts                      11282 non-null  float64\n",
      " 87  SYN_Count                    11282 non-null  float64\n",
      " 88  ACK_Count                    11282 non-null  float64\n",
      " 89  RST_Count                    11282 non-null  float64\n",
      " 90  FIN_Count                    11282 non-null  float64\n",
      " 91  SubLabel                     11282 non-null  object \n",
      "dtypes: float64(90), object(2)\n",
      "memory usage: 41.1+ MB\n",
      "None\n",
      "Destination Port               11282\n",
      "Flow Duration                  11282\n",
      "Total Fwd Packets              11282\n",
      "Total Backward Packets         11282\n",
      "Total Length of Fwd Packets    11282\n",
      "                               ...  \n",
      "SYN_Count                      47227\n",
      "ACK_Count                      47227\n",
      "RST_Count                      47227\n",
      "FIN_Count                      47227\n",
      "SubLabel                       47227\n",
      "Length: 92, dtype: int64\n",
      "Destination Port               0\n",
      "Flow Duration                  0\n",
      "Total Fwd Packets              0\n",
      "Total Backward Packets         0\n",
      "Total Length of Fwd Packets    0\n",
      "                              ..\n",
      "BwdPkts                        0\n",
      "SYN_Count                      0\n",
      "ACK_Count                      0\n",
      "RST_Count                      0\n",
      "FIN_Count                      0\n",
      "Length: 90, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadip\\AppData\\Local\\Temp\\ipykernel_29360\\53214932.py:3: DtypeWarning: Columns (91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.isna().sum())\n",
    "print(np.isinf(df.select_dtypes(include=[np.number])).sum())\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(thresh=int(0.9 * df.shape[1]))  # Keep rows with >=90% non-NaN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e70087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset shape: (47227, 79)\n",
      "🧪 Label distribution:\n",
      "Label\n",
      "PortScan                      5000\n",
      "DDoS                          5000\n",
      "SSH-Patator                   5000\n",
      "DoS Hulk                      5000\n",
      "BENIGN                        5000\n",
      "FTP-Patator                   5000\n",
      "DoS GoldenEye                 5000\n",
      "DoS slowloris                 5000\n",
      "DoS Slowhttptest              5000\n",
      "Web Attack � Brute Force      1507\n",
      "Web Attack � XSS               652\n",
      "Infiltration                    36\n",
      "Web Attack � Sql Injection      21\n",
      "Heartbleed                      11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\", low_memory=False)\n",
    "\n",
    "# === Keep only valid CICIDS-style rows (at least 70 non-NaN feature columns)\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(thresh=70)  # Require at least 70 non-NaN values to retain the row\n",
    "\n",
    "# === Drop any custom extra columns (like 'Duration', 'SubLabel', etc.)\n",
    "extra_cols = [col for col in df.columns if col not in df.columns[:78] and col != \"Label\"]\n",
    "df = df.drop(columns=extra_cols)\n",
    "\n",
    "# Final safety check\n",
    "if df.empty:\n",
    "    raise ValueError(\"❌ Dataframe is still empty after filtering. Please inspect source file.\")\n",
    "else:\n",
    "    print(f\"✅ Cleaned dataset shape: {df.shape}\")\n",
    "    print(f\"🧪 Label distribution:\\n{df['Label'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad96bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 2.6392\n",
      "Epoch 2/20 - Loss: 2.4975\n",
      "Epoch 3/20 - Loss: 2.3708\n",
      "Epoch 4/20 - Loss: 2.2381\n",
      "Epoch 5/20 - Loss: 2.1116\n",
      "Epoch 6/20 - Loss: 1.9883\n",
      "Epoch 7/20 - Loss: 1.8725\n",
      "Epoch 8/20 - Loss: 1.7604\n",
      "Epoch 9/20 - Loss: 1.6547\n",
      "Epoch 10/20 - Loss: 1.5545\n",
      "Epoch 11/20 - Loss: 1.4627\n",
      "Epoch 12/20 - Loss: 1.3774\n",
      "Epoch 13/20 - Loss: 1.2988\n",
      "Epoch 14/20 - Loss: 1.2236\n",
      "Epoch 15/20 - Loss: 1.1499\n",
      "Epoch 16/20 - Loss: 1.0852\n",
      "Epoch 17/20 - Loss: 1.0165\n",
      "Epoch 18/20 - Loss: 0.9602\n",
      "Epoch 19/20 - Loss: 0.9039\n",
      "Epoch 20/20 - Loss: 0.8522\n",
      "\n",
      "📊 Evaluation Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    BENIGN       0.99      0.55      0.71       999\n",
      "                      DDoS       0.83      0.95      0.88      1000\n",
      "             DoS GoldenEye       0.83      0.96      0.89      1000\n",
      "                  DoS Hulk       0.89      0.81      0.85       997\n",
      "          DoS Slowhttptest       0.91      0.82      0.86      1000\n",
      "             DoS slowloris       0.88      0.51      0.65      1000\n",
      "               FTP-Patator       0.82      1.00      0.90      1000\n",
      "                Heartbleed       0.00      0.00      0.00         2\n",
      "              Infiltration       0.00      0.00      0.00         7\n",
      "                  PortScan       0.74      0.99      0.85      1000\n",
      "               SSH-Patator       0.83      1.00      0.91      1000\n",
      "  Web Attack � Brute Force       0.57      0.83      0.68       302\n",
      "Web Attack � Sql Injection       0.00      0.00      0.00         4\n",
      "          Web Attack � XSS       0.00      0.00      0.00       130\n",
      "\n",
      "                  accuracy                           0.83      9441\n",
      "                 macro avg       0.59      0.60      0.58      9441\n",
      "              weighted avg       0.84      0.83      0.82      9441\n",
      "\n",
      "✅ Saved model as ids_cnn_multiclass.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === Load dataset ===\n",
    "# === Clean the dataset properly ===\n",
    "df = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\", low_memory=False)\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(thresh=70)\n",
    "\n",
    "# Keep CICIDS-style 78 features + Label\n",
    "columns_to_keep = list(df.columns[:78]) + [\"Label\"]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Drop columns with constant values or all zeros\n",
    "df = df.loc[:, (df != df.iloc[0]).any()]  # removes constant columns\n",
    "df = df.loc[:, (df != 0).any()]           # removes all-zero columns\n",
    "\n",
    "# Drop any rows with remaining NaNs\n",
    "df = df.dropna()\n",
    "\n",
    "# Final check\n",
    "if df.empty:\n",
    "    raise ValueError(\"🚨 Cleaned DataFrame is empty. Check for over-filtering.\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"Label\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# Clip outlier values (optional, stabilizes learning)\n",
    "X = X.clip(lower=X.quantile(0.01), upper=X.quantile(0.99), axis=1)\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save preprocessors\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "joblib.dump(list(X.columns), \"features_list.pkl\")\n",
    "\n",
    "# Convert to tensors\n",
    "import torch\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# CNN with dynamic flatten\n",
    "import torch.nn as nn\n",
    "\n",
    "class IDS_CNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, input_dim)\n",
    "            self.flat_dim = self.conv(dummy).view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.conv(x))\n",
    "\n",
    "# Train\n",
    "model = IDS_CNN(X_train_tensor.shape[1], len(label_encoder.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor.unsqueeze(1))\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"❌ NaN loss at epoch {epoch} — aborting\")\n",
    "        break\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}/20 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.unsqueeze(1))\n",
    "    y_pred = torch.argmax(preds, dim=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n📊 Evaluation Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# TorchScript export\n",
    "torch.jit.trace(model, X_test_tensor[:1].unsqueeze(1)).save(\"ids_cnn_multiclass.pt\")\n",
    "print(\"✅ Saved model as ids_cnn_multiclass.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0bdd1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Heartbleed: 11 rows\n",
      "✅ SQLi: 21 rows\n",
      "✅ Infiltration: 36 rows\n",
      "✅ XSS: 652 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset with encoding fallback\n",
    "df = pd.read_csv(\"CICIDS2017_Multiclass_Balanced_5k_with_portscan_variants.csv\", encoding='utf-8', low_memory=False)\n",
    "\n",
    "# Clean and sanitize\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Label'] = df['Label'].astype(str).str.strip()\n",
    "\n",
    "# Define label fragments to match using substring\n",
    "rare_fragments = {\n",
    "    \"Heartbleed\": \"Heartbleed\",\n",
    "    \"SQLi\": \"Sql Injection\",\n",
    "    \"Infiltration\": \"Infiltration\",\n",
    "    \"XSS\": \"XSS\"\n",
    "}\n",
    "\n",
    "# Extract and save\n",
    "for key, fragment in rare_fragments.items():\n",
    "    subset = df[df[\"Label\"].str.contains(fragment, case=False, na=False)]\n",
    "    print(f\"✅ {key}: {len(subset)} rows\")\n",
    "    subset.to_csv(f\"RARE_{key}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4800044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No numeric columns available for scaling. Please check the input data.\n"
     ]
    }
   ],
   "source": [
    "# === Load and prepare Heartbleed data safely ===\n",
    "df = pd.read_csv(\"RARE_Heartbleed.csv\")\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop columns that are entirely NaN\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Fill remaining NaNs with median values (per column)\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Drop non-numeric columns before scaling\n",
    "X = df.select_dtypes(include=[np.number])\n",
    "\n",
    "if X.empty:\n",
    "\tprint(\"❌ No numeric columns available for scaling. Please check the input data.\")\n",
    "\tX_scaled = np.array([])\n",
    "\tX_tensor = torch.tensor([])\n",
    "else:\n",
    "\tscaler = StandardScaler()\n",
    "\tX_scaled = scaler.fit_transform(X)\n",
    "\tX_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d250c9f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 90)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 13\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m X_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# === GAN model definition ===\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\aadip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 90)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# === Load and prepare Heartbleed data ===\n",
    "df = pd.read_csv(\"RARE_Heartbleed.csv\")\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "X = df.drop(columns=[\"Label\", \"SubLabel\"]) if \"SubLabel\" in df.columns else df.drop(columns=[\"Label\"])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "# === GAN model definition ===\n",
    "latent_dim = 32\n",
    "data_dim = X_tensor.shape[1]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, data_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "criterion = nn.BCELoss()\n",
    "g_opt = torch.optim.Adam(G.parameters(), lr=0.0005)\n",
    "d_opt = torch.optim.Adam(D.parameters(), lr=0.0005)\n",
    "\n",
    "# === Training loop ===\n",
    "epochs = 3000\n",
    "batch_size = min(32, len(X_tensor))\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, len(X_tensor), batch_size)\n",
    "    real_samples = X_tensor[idx]\n",
    "    real_labels = torch.ones(batch_size, 1)\n",
    "    fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "    # Train Discriminator\n",
    "    z = torch.randn(batch_size, latent_dim)\n",
    "    fake_samples = G(z)\n",
    "    d_real = D(real_samples)\n",
    "    d_fake = D(fake_samples.detach())\n",
    "    d_loss = criterion(d_real, real_labels) + criterion(d_fake, fake_labels)\n",
    "    D.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_opt.step()\n",
    "\n",
    "    # Train Generator\n",
    "    z = torch.randn(batch_size, latent_dim)\n",
    "    fake_samples = G(z)\n",
    "    d_fake = D(fake_samples)\n",
    "    g_loss = criterion(d_fake, real_labels)\n",
    "    G.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_opt.step()\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"[{epoch+1}/{epochs}] D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# === Generate synthetic samples ===\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(100, latent_dim)\n",
    "    synthetic = G(z).numpy()\n",
    "    synthetic_unscaled = scaler.inverse_transform(synthetic)\n",
    "    df_synth = pd.DataFrame(synthetic_unscaled, columns=X.columns)\n",
    "    df_synth[\"Label\"] = \"Heartbleed\"\n",
    "    df_synth[\"SubLabel\"] = \"Synthetic\"\n",
    "\n",
    "df_synth.to_csv(\"SYN_Heartbleed.csv\", index=False)\n",
    "print(\"✅ Saved: SYN_Heartbleed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a61324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
